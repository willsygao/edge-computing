## 变更目标
- 所有度量（时延、能耗、移动机器人效用、服务器效用、总目标OG）只在任务“完成或失败”时计算并计入奖励与可视化。
- 奖励函数改为“最大化总目标OG”，同时对失败任务加入额外惩罚。

## 核心改动
- 延迟计算到任务结束：
  - 本地任务：仍为即时完成（`local_cost`），此时即刻视为“结束”，度量生效。
  - 边缘任务：在 `core.py:234-251` 的回查逻辑中，当服务端标记 `COMPLETED/FAILED` 时，才计算最终度量：
    - `time_cur = trans_t + (completion_slot - creation_slot) * slot_time`（失败则使用 `failure_slot` 或 `max_tolerance_delay * slot_time`）。
    - `energy_cur = trans_e`（机器人侧仅统计发射能耗；如需计入服务器能耗，后续可扩展）。
- 成本归一化（效用中的 `C_norm`）聚合到任务结束：
  - 边缘：在 `PriorityQueueServer._allocate_resources_by_computation` 中，为每个任务维护 `alloc_traj`（每时隙分配比例），任务结束时以均值或加权和作为 `C_norm_edge`。
  - 本地：`C_norm_local = (v_alloc · f_UE) / max_local_frequency`（一次性）。
- 满意度 `S_norm` 在任务结束时计算：`S_norm = clamp(1 − time_cur / τ, 0, 1)`。
- 移动机器人效用：仍为 `U_i^a(t)=θ·S_norm − (1−θ)·C_norm`，但仅在结束时更新并纳入累计。
- 服务器效用：增量基于当步结束的任务数与队列状态（`ΔC, ΔF, Q, N`），继续用 `U_server_s = a·ΔC − b·ΔF − c·Q − d·N`，其中 `ΔC`/`ΔF` 为当步新增完成/失败任务。

## 奖励与总目标
- 每时隙计算 `OG_step = Σ_{结束任务} φ_i^a · p_i^b · (U_i^a + U_j^i)`，只有当步“结束”的任务参与。
- 奖励为共享：每个智能体的奖励 `= OG_step / n − fail_penalty_if_this_agent_failed_in_step`。
- 代码落点：
  - `onpolicy/envs/my/environment.py:95-120`：成本计算 → 结束任务度量/OG计算缓存 → 奖励按缓存分发。
  - `onpolicy/envs/my/scenarios/simple_spread.py:109-111`：奖励函数使用 `world._cache_og_total / n` 及失败惩罚。

## 数据结构与实现细节
- `ComputeTask` 扩展字段：`failure_slot`（失败时刻）、`alloc_traj`（资源分配轨迹，列表）。
- `PriorityQueueServer._allocate_resources_by_computation`：每次为任务分配资源时，将比例追加到该任务的 `alloc_traj`。
- `MecWorld`：
  - 在回查完成/失败处，计算并写入 `agent.state.time_cur/energy_cur`（结束时才赋值）。
  - `agent_utility` 仅在任务 `state in (2,3)` 时返回有效值，否则返回 0。
  - `total_objective` 仅聚合当步结束任务；并缓存到 `world._cache_og_total` 供奖励与可视化使用。

## 可视化
- `visualize.update(..., metrics)` 改为使用“截至当前的结束任务”聚合：
  - `agent_utility_mean`：以已结束任务的效用为样本。
  - `server_utility`：仍按时隙绘制，但 `ΔC/ΔF` 来自结束任务。
  - `og_total`：当步结束任务的 `OG_step`，生成 `og_total_over_time.png`。

## 兼容与验证
- 不改变现有动作/空间/训练流程；奖励从能耗惩罚改为OG最大化并含失败惩罚。
- 短步运行验证：
  - 本地 `visual_out` 的三图随训练刷新。
  - 失败任务时奖励曲线出现下降；完成任务时出现正向提升。
  - 打印若干步的 `completion_slot/alloc_traj` 以检查聚合正确性。